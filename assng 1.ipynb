{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121661ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdad168",
   "metadata": {},
   "source": [
    "Question 1 - display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5694a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "\n",
    "# page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "header_tags = [] # empty list\n",
    "for header in soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"]):\n",
    "    header_tags.append(header.name+\" \"+header.text.strip())\n",
    "    \n",
    "# print all header_tags\n",
    "header_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.imdb.com/list/ls091520106/\"\n",
    "page2 = requests.get(url)\n",
    "\n",
    "#see page content\n",
    "soup2 = BeautifulSoup(page2.content)\n",
    "\n",
    "# top Movies name\n",
    "name = soup2.find_all(\"h3\", class_=\"lister-item-header\")\n",
    "# get text from movie name web elements\n",
    "movies_name = [] #empty list\n",
    "for i in name:\n",
    "    for j in i.find_all(\"a\"):\n",
    "        movies_name.append(j.text)\n",
    "\n",
    "\n",
    "# Year of release\n",
    "year = soup2.find_all(\"span\",class_=\"lister-item-year text-muted unbold\")\n",
    "year_of_release = [] #empty list\n",
    "for k in year:\n",
    "    a=k.text.replace('(','')\n",
    "    year_of_release.append(a.replace(')','')) \n",
    "\n",
    "\n",
    "      \n",
    "# IMDB Rating\n",
    "rating = soup2.find_all(\"div\",class_=\"ipl-rating-star small\")\n",
    "\n",
    "# scrape text from rating web element\n",
    "IMDB_rating = [] #empty list\n",
    "for i in rating:\n",
    "      IMDB_rating.append(float(i.text))\n",
    "# Make data frame of top 100 movies on IMDB\n",
    "IMDB_top_100=pd.DataFrame({})\n",
    "IMDB_top_100['movies_name']=movies_name\n",
    "IMDB_top_100['year_of_release']=year_of_release\n",
    "IMDB_top_100['IMDB_rating']=IMDB_rating  \n",
    "IMDB_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.imdb.com/list/ls009997493/\"\n",
    "page3 = requests.get(url)\n",
    "\n",
    "# check the page content\n",
    "soup3 = BeautifulSoup(page3.content)\n",
    "\n",
    "# top Movies name\n",
    "name = soup3.find_all(\"h3\", class_=\"lister-item-header\")\n",
    "# get text from movie name web elements\n",
    "movies_name = [] #empty list\n",
    "for i in name:\n",
    "    for j in i.find_all(\"a\"):\n",
    "        movies_name.append(j.text)\n",
    "\n",
    "\n",
    "# Year of release\n",
    "year = soup3.find_all(\"span\",class_=\"lister-item-year text-muted unbold\")\n",
    "year_of_release = [] #empty list\n",
    "\n",
    "for k in year:\n",
    "    a=k.text.replace('(','')\n",
    "    year_of_release.append(a.replace(')','')) \n",
    "    \n",
    "\n",
    "# IMDB Rating\n",
    "rating = soup3.find_all(\"div\",class_=\"ipl-rating-star small\")\n",
    "\n",
    "# scrape text from rating web element\n",
    "IMDB_rating = [] #empty list\n",
    "for i in rating:\n",
    "      IMDB_rating.append(float(i.text))\n",
    "# Make data frame of top 100 India imdb movies\n",
    "indian_top_100=pd.DataFrame({})\n",
    "indian_top_100['movies_name']=movies_name\n",
    "indian_top_100['year_of_release']=year_of_release\n",
    "indian_top_100['IMDB_rating']=IMDB_rating\n",
    "indian_top_100    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "page5 = requests.get(url)\n",
    "# see content in page5\n",
    "soup5 = BeautifulSoup(page5.content)\n",
    "#scrape team names\n",
    "team = soup5.find_all(\"span\",class_='u-hide-phablet')\n",
    "team_name = []\n",
    "for i in team:\n",
    "    team_name.append(i.text)\n",
    "matches = [] #empty list\n",
    "points = [] #empty list\n",
    "ratings = [] #empty list\n",
    "new_list = [] #empty list\n",
    "\n",
    "for i in soup5.find_all(\"td\",class_='rankings-block__banner--matches'): # first place team number of matches\n",
    "    matches.append(i.text)\n",
    "for i in soup5.find_all(\"td\",class_='rankings-block__banner--points'):# first place team points\n",
    "    points.append(i.text)\n",
    "for i in soup5.find_all(\"td\",class_='rankings-block__banner--rating u-text-right'):# first place team ratings\n",
    "    ratings.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup5.find_all(\"td\",class_='table-body__cell u-center-text'):# other teams number of matches and points\n",
    "    new_list.append(i.text)\n",
    "for i in range(0,len(new_list)-1,2):\n",
    "    matches.append(new_list[i]) # other teams matches\n",
    "    points.append(new_list[i+1]) # other teams points\n",
    "for i in soup5.find_all(\"td\",class_='table-body__cell u-text-right rating'):# other teams ratings\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "# Make data frame of top 10 ICC teams\n",
    "icc=pd.DataFrame({})\n",
    "icc['Team_name']=team_name[:10]\n",
    "icc['Matches']=matches[:10]\n",
    "icc['Points']=points[:10]\n",
    "icc['Ratings']=ratings[:10]\n",
    "icc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597df880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "page6 = requests.get(url)\n",
    "# see content in page6\n",
    "soup6 = BeautifulSoup(page6.content)\n",
    "players = [] #empty list\n",
    "team_name = [] #empty list\n",
    "rating = [] #empty list\n",
    "\n",
    "for i in soup6.find_all(\"div\",class_='rankings-block__banner--name-large'): # first place player name\n",
    "    players.append(i.text)\n",
    "for i in soup6.find_all(\"div\",class_='rankings-block__banner--nationality'): # first place player team name\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup6.find_all(\"div\",class_='rankings-block__banner--rating'): # first place player rating\n",
    "    rating.append(i.text)\n",
    "for i in soup6.find_all(\"td\",class_='table-body__cell rankings-table__name name'):# players name\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "for i in soup6.find_all(\"span\",class_='table-body__logo-text'): # players team name\n",
    "    team_name.append(i.text)\n",
    "for i in soup6.find_all(\"td\",class_='table-body__cell rating'): # players rating\n",
    "    rating.append(i.text)\n",
    "# Make data frame of top 10 ICC Batsmen\n",
    "Batsmen=pd.DataFrame({})\n",
    "Batsmen['Player']=players[:10]\n",
    "Batsmen['Team']=team_name[:10]\n",
    "Batsmen['Rating']=rating[:10]\n",
    "Batsmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ea981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "page7 = requests.get(url)\n",
    "\n",
    "# see content in page7\n",
    "soup7 = BeautifulSoup(page7.content)\n",
    "\n",
    "players = [] #empty list\n",
    "team_name = [] #empty list  \n",
    "rating = [] #empty list \n",
    "\n",
    "for i in soup7.find_all(\"div\",class_='rankings-block__banner--name-large'): # first place player name\n",
    "    players.append(i.text)\n",
    "for i in soup7.find_all(\"div\",class_='rankings-block__banner--nationality'): # first place player team name\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup7.find_all(\"div\",class_='rankings-block__banner--rating'): # first place player rating\n",
    "    rating.append(i.text)\n",
    "for i in soup7.find_all(\"td\",class_='table-body__cell rankings-table__name name'):# players name\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "for i in soup7.find_all(\"span\",class_='table-body__logo-text'): # players team name\n",
    "    team_name.append(i.text)\n",
    "for i in soup7.find_all(\"td\",class_='table-body__cell rating'): # players rating\n",
    "    rating.append(i.text)\n",
    "# Make data frame of top 10 ICC bowlers\n",
    "bowlers=pd.DataFrame({})\n",
    "bowlers['Player']=players[:10]\n",
    "bowlers['Team']=team_name[:10]\n",
    "bowlers['Rating']=rating[:10]\n",
    "bowlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854de175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "page8 = requests.get(url)\n",
    "#see content in page8\n",
    "soup8 = BeautifulSoup(page8.content)\n",
    "#scrape team names\n",
    "womens_team = soup8.find_all(\"span\",class_='u-hide-phablet')\n",
    "womens_team_name = []\n",
    "for i in womens_team:\n",
    "    womens_team_name.append(i.text)\n",
    "womens_matches = [] #empty list\n",
    "womens_points = [] #empty list\n",
    "womens_ratings = [] #empty list\n",
    "womens_new_list = [] #empty list\n",
    "for i in soup8.find_all(\"td\",class_='rankings-block__banner--matches'): # first place team number of matches\n",
    "    womens_matches.append(i.text)\n",
    "for i in soup8.find_all(\"td\",class_='rankings-block__banner--points'):# first place team points\n",
    "    womens_points.append(i.text)\n",
    "for i in soup8.find_all(\"td\",class_='rankings-block__banner--rating u-text-right'):# first place team ratings\n",
    "    womens_ratings.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup8.find_all(\"td\",class_='table-body__cell u-center-text'):# other teams number of matches and points\n",
    "    womens_new_list.append(i.text)\n",
    "for i in range(0,len(womens_new_list)-1,2):\n",
    "    womens_matches.append(womens_new_list[i]) # other teams matches\n",
    "    womens_points.append(womens_new_list[i+1]) # other teams points\n",
    "for i in soup8.find_all(\"td\",class_='table-body__cell u-text-right rating'):# other teams number of matches and ratings\n",
    "    womens_ratings.append(i.text)\n",
    "# Make data frame of top 10 ICC teams\n",
    "womens_icc=pd.DataFrame({})\n",
    "womens_icc['Team_name']=womens_team_name[:10]\n",
    "womens_icc['Matches']=womens_matches[:10]\n",
    "womens_icc['Points']=womens_points[:10]\n",
    "womens_icc['Ratings']=womens_ratings[:10]\n",
    "womens_icc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "page9 = requests.get(url)\n",
    "# see content in page9\n",
    "soup9 = BeautifulSoup(page9.content)\n",
    "players = [] #empty list\n",
    "team_name = [] #empty list\n",
    "rating = [] #empty list\n",
    "\n",
    "for i in soup9.find_all(\"div\",class_='rankings-block__banner--name-large'): # first place player name\n",
    "    players.append(i.text)\n",
    "for i in soup9.find_all(\"div\",class_='rankings-block__banner--nationality'): # first place player team name\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup9.find_all(\"div\",class_='rankings-block__banner--rating'): # first place player rating\n",
    "    rating.append(i.text)\n",
    "for i in soup9.find_all(\"td\",class_='table-body__cell rankings-table__name name'):# players name\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "for i in soup9.find_all(\"span\",class_='table-body__logo-text'): # players team name\n",
    "    team_name.append(i.text)\n",
    "for i in soup9.find_all(\"td\",class_='table-body__cell rating'): # players rating\n",
    "    rating.append(i.text)\n",
    "# Make data frame of top 10 Women's ODI Batting Rankings\n",
    "top_players=pd.DataFrame({})\n",
    "top_players['Player']=players[:10]\n",
    "top_players['Team']=team_name[:10]\n",
    "top_players['Rating']=rating[:10]\n",
    "top_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "page10 = requests.get(url)\n",
    "\n",
    "# see content in page10\n",
    "soup10 = BeautifulSoup(page10.content)\n",
    "\n",
    "players = [] #empty list\n",
    "team_name = [] #empty list  \n",
    "rating = [] #empty list \n",
    "\n",
    "for i in soup10.find_all(\"div\",class_='rankings-block__banner--name-large'): # first place player name\n",
    "    players.append(i.text)\n",
    "for i in soup10.find_all(\"div\",class_='rankings-block__banner--nationality'): # first place player team name\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup10.find_all(\"div\",class_='rankings-block__banner--rating'): # first place player rating\n",
    "    rating.append(i.text)\n",
    "for i in soup10.find_all(\"td\",class_='table-body__cell rankings-table__name name'):# players name\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "for i in soup10.find_all(\"span\",class_='table-body__logo-text'): # players team name\n",
    "    team_name.append(i.text)\n",
    "for i in soup10.find_all(\"td\",class_='table-body__cell rating'): # players rating\n",
    "    rating.append(i.text)\n",
    "# Make data frame of top 10 ICC Women's ODI All-Rounder Rankings\n",
    "all_rounder=pd.DataFrame({})\n",
    "all_rounder['Player']=players[:10]\n",
    "all_rounder['Team']=team_name[:10]\n",
    "all_rounder['Rating']=rating[:10]\n",
    "all_rounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.amazon.in/s?k=Mobile+phones+under+20000&ref=nb_sb_noss_2\"\n",
    "page11 = requests.get(url)\n",
    "#see content in page11\n",
    "soup11 = BeautifulSoup(page11.content)\n",
    "product_name = []  #empty list\n",
    "price = []    #empty list\n",
    "rating = []   #empty list\n",
    "img_url = []   #empty list\n",
    "\n",
    "\n",
    "# Scrape product name\n",
    "for i in soup11.find_all(\"span\",class_=\"a-size-medium a-color-base a-text-normal\"):\n",
    "    product_name.append(i.text)\n",
    "\n",
    "# Scrape product price\n",
    "for i in soup11.find_all(\"span\",class_=\"a-price-whole\"):\n",
    "    price.append(i.text)\n",
    "for i in soup11.find_all(\"span\",class_=\"a-icon-alt\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "# Scrape images url\n",
    "for i in soup11.find_all(\"img\",class_=\"s-image\"):\n",
    "    img_url.append(i.get(\"src\"))\n",
    "    \n",
    "# Make data frame mobile phones under Rs. 20,000 listed on Amazon.in\n",
    "mobile_phones=pd.DataFrame({})\n",
    "mobile_phones['product_name']=product_name[:16]\n",
    "mobile_phones['price']=price[:16]\n",
    "mobile_phones['Rating']=rating[:16]\n",
    "mobile_phones['img_url'] = img_url[:16]\n",
    "mobile_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0\"\n",
    "page14 = requests.get(url)\n",
    "# see content in page14\n",
    "soup14 = BeautifulSoup(page14.content)\n",
    "\n",
    "house = []  #empty list\n",
    "location = []  #empty list\n",
    "Area = []  #empty list\n",
    "EMI = []   #empty list\n",
    "price = []   #empty list\n",
    "\n",
    "\n",
    "#scrape house title \n",
    "houses = soup14.find_all(\"h2\", class_=\"heading-6 font-semi-bold nb__1AShY\")\n",
    "for i in houses:\n",
    "    house.append(i.text)\n",
    "    \n",
    "#scrape house location     \n",
    "loc = soup14.find_all('div',class_=\"nb__2CMjv\")\n",
    "for i in loc:\n",
    "    location.append(i.text)\n",
    "    \n",
    "#scrape house location     \n",
    "area = soup14.find_all('div',class_=\"nb__3oNyC\")\n",
    "for i in area:\n",
    "    Area.append(i.text)\n",
    "    \n",
    "\n",
    "full_info = []\n",
    "detail = soup14.find_all('div',class_=\"font-semi-bold heading-6\")\n",
    "for i in detail:\n",
    "    full_info.append(i.text)\n",
    "\n",
    "#EMI\n",
    "for i in range(1,len(full_info),3):\n",
    "    EMI.append(full_info[i])\n",
    "\n",
    "# price\n",
    "for i in range(2,len(full_info),3):\n",
    "    price.append(full_info[i])\n",
    "\n",
    "# Make data frame\n",
    "nobroker=pd.DataFrame({})\n",
    "nobroker['House']=house\n",
    "nobroker['Area']=Area\n",
    "nobroker['location']=location\n",
    "nobroker['EMI']=EMI\n",
    "nobroker['price']=price\n",
    "nobroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78edc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "page = requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")\n",
    "# page content\n",
    "soup=BeautifulSoup(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[]\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-info cursor\"):\n",
    "    name.append(i.text)\n",
    "location=[]\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "\n",
    "price = []\n",
    "cuisine = []\n",
    "for i in soup.find_all(\"span\", class_=\"double-line-ellipsis\"):\n",
    "    price.append(i.text.split('|')[0])\n",
    "    cuisine.append(i.text.split('|')[1])\n",
    "\n",
    "rating=[]\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-rating rating-3\"):\n",
    "    rating.append(i.text)\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-rating rating-4\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "images = []\n",
    "for i in soup.find_all(\"img\", class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n",
    "\n",
    "print(len(name), len(location), len(price), len(cuisine), len(rating), len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa38942",
   "metadata": {},
   "outputs": [],
   "source": [
    "DineOut=pd.DataFrame({})\n",
    "DineOut['Restaurant Name']=name\n",
    "DineOut['Location']=location\n",
    "DineOut['Price']=price \n",
    "DineOut['Cuisine']=cuisine  \n",
    "DineOut['Rating']=rating  \n",
    "DineOut['IMAGES']=images\n",
    "DineOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.tutiempo.net/delhi.html?data=last-24-hours'\n",
    "page = requests.get(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlContent= page.content\n",
    "soup= BeautifulSoup(htmlContent,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list\n",
    "Temperature = []\n",
    "Wind_km_hr = []\n",
    "Humidity= []\n",
    "Pressure_hPA = []\n",
    "Weather_condition = []\n",
    "Hour=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8936ece5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-672ee8f983f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m't Temp'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#fetching temperature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wind'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#fetching wind\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhumid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hr'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#fetching humidity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpressure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'prob'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#fetching pressure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "temp = soup.find_all ('td',class_='t Temp') #fetching temperature\n",
    "wind = soup.find_all ('td',class_='wind') #fetching wind\n",
    "humid = soup.find_all ('td',class_='hr') #fetching humidity\n",
    "pressure = soup.find_all ('td',class_='prob') #fetching pressure\n",
    "\n",
    "for i in range(0,24):\n",
    "    Temperature.append(temp[i].get_text().replace(\"\\n\",\"\"))\n",
    "    Wind_km_hr.append(wind[i].get_text().replace(\"\\n\",\"\"))\n",
    "    Humidity.append(humid[i].get_text().replace(\"\\n\",\"\"))\n",
    "    Pressure_hPA.append(pressure[i].get_text().replace(\"\\n\",\"\"))\n",
    "    #Weather_condition.append(weather[i].get_text().replace(\"\\n\",\"\"))\n",
    "\n",
    "#since hours is not present inside any class so using previous sibling tag to fetch hours\n",
    "for i in soup.find_all ('td',class_='t Temp')[0:24]: \n",
    "    if i.previous_sibling.previous_sibling is not None:\n",
    "        Hour.append(i.previous_sibling.previous_sibling.text)\n",
    "    else:\n",
    "        Hour.append(' ')\n",
    "\n",
    "for i in soup.find_all ('td',class_='t Temp')[0:24]: \n",
    "    if i.previous_sibling is not None:\n",
    "        Weather_condition.append(i.previous_sibling.text)\n",
    "    else:\n",
    "        Weather_condition.append(' ')\n",
    "        \n",
    "import pandas as pd\n",
    "data = list(zip(Hour,Temperature,Wind_km_hr,Humidity,Pressure_hPA,Weather_condition))\n",
    "df = pd.DataFrame(data,columns=['Hour','Temperature','Wind_km_hr','Humidity','Pressure_hPA','Weather_condition'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20822c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
